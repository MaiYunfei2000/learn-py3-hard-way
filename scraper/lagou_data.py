"""
ğŸ•·ç»ƒä¹ ï¼šå°è¯•çˆ¬å– lagou ç½‘ä»¥ Python ä¸ºå…³é”®è¯çš„æ‹›è˜ä¿¡æ¯ä¿¡æ¯
"""

import time
import pandas as pd
from datetime import datetime
from selenium.webdriver import Chrome
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support import expected_conditions as EC

# ========================================
# åˆå§‹åŒ–
# ========================================

# æŸ¥æ‰¾çš„èŒä¸šå…³é”®è¯
job_key = 'Python'
# job_key = input("è¯·è¾“å…¥èŒä¸šçš„å…³é”®è¯ï¼ˆå¯ä¸­æ–‡ï¼‰ï¼š")

# è·å–å½“å‰æ—¶é—´
today_time_str = str(datetime.today())[:16]
print(f"\nå½“å‰æ—¶é—´ï¼š{today_time_str}\n")

# # ç¦æ­¢å›¾ç‰‡å’Œ css åŠ è½½
# options = Options()
# prefs = {"profile.managed_default_content_settings.images": 2,'permissions.default.stylesheet':2}
# options.add_experimental_option("prefs", prefs)

# Chrome Driver
ChromeDriverPath = '/opt/WebDriver/bin/chromedriver'
try:
    driver = Chrome(executable_path=ChromeDriverPath, chrome_options=options)
except NameError:
    driver = Chrome(executable_path=ChromeDriverPath)

# ========================================
# æ‰“å¼€ç½‘é¡µ
# ========================================

print("æ­£åœ¨åŠ è½½ç½‘é¡µ......")
print("ï¼ˆæ ‡ç­¾é¡µçš„åœˆåœˆè½¬å®Œæ‰æ„å‘³ç€åŠ è½½å®Œæ¯•ï¼›è§†ç½‘ç»œæ¡ä»¶ï¼Œæœ‰æ—¶å¯èƒ½è¦ç­‰å¾…ç•¥ä¹…ï¼‰")
driver.get(f"https://www.lagou.com/jobs/list_{job_key}/")
print("\nç½‘é¡µåŠ è½½å®Œæ¯•ã€‚è¯·å…ˆç™»å½•ã€‚")

# ç­‰å¾…è¿™ä¸ªè«åå…¶å¦™çš„çº¢åŒ…å¹¿å‘Šå‡ºç°
element = WebDriverWait(driver, timeout=10).until(
        EC.presence_of_element_located((By.XPATH, '/html/body/div[9]/div/div[2]')))
# ç„¶åæŠŠè¿™è«åå…¶å¦™çš„çº¢åŒ…å¹¿å‘Šç»™ç‚¹æ‰ï¼ˆç‚¹å‡»â€œç»™ä¹Ÿä¸è¦â€æŒ‰é’®ï¼‰
closing_button = driver.find_element_by_xpath('/html/body/div[9]/div/div[2]')
closing_button.click()

# ç‚¹å¼€ç™»å½•ç•Œé¢
login_access = driver.find_element_by_xpath('//*[@id="lg_tbar"]/div/div[2]/ul/li[3]/a')
login_access.click()

def normal_login():
    """
    æ™®é€šç™»å½•ï¼ˆæœªèƒ½è§£å†³è‡ªåŠ¨éªŒè¯é—®é¢˜ï¼‰
    """
    # è¾“å…¥è´¦å·ä¿¡æ¯
    id_input = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div/div/div[2]/div[3]/div[1]/div/div[1]/form/div[1]/div/input')
    pass_input = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div/div/div[2]/div[3]/div[1]/div/div[1]/form/div[2]/div/input')
    user_id = 'NONE'
    password = 'NONE'
    id_input.send_keys(user_id)
    pass_input.send_keys(password)

    # ç‚¹å‡»â€œç™»å½•â€
    login_button = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div/div/div[2]/div[3]/div[2]/div[2]/div[2]')
    login_button.click()

def qq_login():
    # ç½‘ç»œæ¥å£è¿‡äºåƒåœ¾ï¼Œæ”¾å¼ƒã€‚ã€‚ã€‚
    """
    ä½¿ç”¨ QQ è´¦å·ç™»å½•
    """
    # é€‰æ‹© QQ è´¦å·ç™»å½•å…¥å£
    # qq_login = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div/div/div[2]/div[3]/div[4]/div/a[3]')
    qq_login = driver.find_element_by_xpath('//*[@id="img_out_572353500"]')
    qq_login.click()

    # ä½¿ç”¨è´¦å·å¯†ç ç™»å½•
    not_discovered = True
    while not_discovered:
        try:
            id_login = driver.find_element_by_xpath('//*[@id="switcher_plogin"]')
            not_discovered = False # //*[@id="switcher_plogin"]
        except NoSuchElementException:
            print("Still trying...")
            time.sleep(0.5)
    id_login.click()

    # ç‚¹å‡»æˆæƒå¹¶ç™»å½•
    authorization = driver.find_element_by_xpath('//*[@id="login_button"]')
    authorization.click()

# qq_login()

# # å†è§£å†³ä¸€æ¬¡çº¢åŒ…å¹¿å‘Š
# closing_button = driver.find_element_by_xpath('/html/body/div[9]/div/div[2]')
# closing_button.click()

probe = input("è¯·æ‰‹åŠ¨ç™»å½•ï¼Œç„¶åè¾“å…¥ä»»æ„å­—ç¬¦ä»¥ç»§ç»­ã€‚")

# è·å–æ‹›è˜ä¿¡æ¯çš„æ€»é¡µæ•°
page_num = driver.find_element_by_xpath('//*[@id="s_position_list"]/div[2]/div/span[5]').text
page_num = int(page_num)

# ========================================
# çˆ¬å–æ‰€æœ‰ä¿¡æ¯
# ========================================

def scrape_and_process(inner_interval=0.05, outer_interval=1):
    """
    input:
          inner_interval(int): time interval of scraping an element
          outer_interval(int): time interval of opening a new page
    output:
          df(pandas.DataFrame): cummulated information
    """

    list_job_name = []
    list_city = []
    list_district = []
    list_time_published = []
    list_salary = []
    list_experience = []
    list_education = []
    list_company = []
    list_domain = []
    list_financing = []
    list_scale = []
    list_keyword = []
    list_remark = []

    for page_i in range(1, page_num+1):
        print("\n# " + "=" * 50)
        print(f"# æ­£åœ¨è¯»å–ç¬¬ {page_i:>2} é¡µä¿¡æ¯...")
        print("# " + "=" * 50)

        # ä½¿ç”¨ class_name æŠ“å–æ‰€æœ‰æ‹›è˜ä¿¡æ¯èŠ‚ç‚¹
        all_elements = driver.find_elements_by_class_name('con_list_item')
        # éå†æ¯é¡µçš„åˆ—è¡¨å…ƒç´ ï¼‰
        for element_i in range(len(all_elements)):

            # ---------------------------------
            # æ¡†å®šç‰¹å®šå…ƒç´ ï¼Œæ‘˜å–æ•´æ¡æ–‡æœ¬ï¼ˆå„ç§æ‹›è˜ä¿¡æ¯ï¼‰
            # ---------------------------------

            element = all_elements[element_i]

            overall_text = element.text

            # æ‹†è§£å­—ç¬¦ä¸²
            text_list = overall_text.split('\n')

            print(text_list)

            """
            åŸå§‹ä¿¡æ¯ï¼š
            | èŒä¸šå | åœ°åŒº | å‘å¸ƒæ—¶é—´ | æœˆè–ª&ç»éªŒéœ€æ±‚&å­¦å†éœ€æ±‚ | å…¬å¸ | å…¬å¸é¢†åŸŸ&èèµ„è½®æ¬¡&äººå‘˜è§„æ¨¡ | å…³é”®è¯ | å¤‡æ³¨ |

            èŒƒä¾‹æ–‡æœ¬ï¼š
            'pythonå¼€å‘å·¥ç¨‹å¸ˆ\n[ä¸Šæµ·Â·å¾æ±‡åŒº]\n2020-08-03\n8k-16k ç»éªŒ1-3å¹´ / æœ¬ç§‘\nä¸Šæµ·æ±Ÿç…¦ä¿¡æ¯ç§‘æŠ€\nç§»åŠ¨äº’è”ç½‘,ç¡¬ä»¶ / ä¸éœ€è¦èèµ„ / 15-50äºº\nPython\nâ€œåŒäº‹éå¸¸nice ä¿Šç”·ç¾å¥³ é¢†å¯¼ä¹Ÿå¾ˆniceâ€
            'pythonå¼€å‘å·¥ç¨‹å¸ˆ\n[åŒ—äº¬Â·æµ·æ·€åŒº]\n1å¤©å‰å‘å¸ƒ\n20k-35k ç»éªŒ1-3å¹´ / æœ¬ç§‘\nå­—èŠ‚è·³åŠ¨\næ–‡å¨±ä¸¨å†…å®¹ / Cè½® / 2000äººä»¥ä¸Š\nåç«¯å¼€å‘\nâ€œå…­é™©ä¸€é‡‘ï¼Œé«˜è–ªæœŸæƒï¼Œå¼¹æ€§å·¥ä½œï¼Œå…è´¹ä¸‰é¤â€'
            'python', '[è¥¿å®‰Â·é«˜æ–°æŠ€â€¦]', '3å¤©å‰å‘å¸ƒ', '5k-10k ç»éªŒ3-5å¹´ / å¤§ä¸“', 'é¼æ‹“ç§‘æŠ€', 'ä¿¡æ¯å®‰å…¨,äººå·¥æ™ºèƒ½ / æœªèèµ„ / 50-150äºº', 'â€œäº”é™©ä¸€é‡‘ã€åŒä¼‘ã€åŠ ç­å°‘ã€ç¨³å®šæ€§é«˜â€'
            """

            # ---------------------------------
            # è·å–å…·ä½“ä¿¡æ¯
            # ---------------------------------

            job_name = text_list[0]

            location = text_list[1].strip('[]').split('Â·')
            city = location[0]
            try:
                district = location[1]
            except IndexError:
                district = 'æ— '

            time_published = text_list[2] # æœ‰å¾…è¿›ä¸€æ­¥å¤„ç†ä¸ºâ€œç»å¯¹åæ ‡â€

            salary_et_al = ''.join(text_list[3].split(' /')).split(' ')
            salary = salary_et_al[0]
            experience = salary_et_al[1]
            education = salary_et_al[2]

            company = text_list[4]

            domain_et_al = text_list[5].split('/')
            domain = domain_et_al[0].strip()
            financing = domain_et_al[1].strip()
            scale = domain_et_al[2].strip()

            keyword = ';'.join(text_list[6].split(" "))
            try:
                assert keyword[0] != "â€œ" and keyword[-1] != "â€"
            except AssertionError:
                print(f"ç¬¬{page_i}é¡µç¬¬{element_i+1}è¡Œæ•°æ®å¯èƒ½æœ‰é—®é¢˜")
                keyword = "æ— "

            remark = text_list[-1]
            try:
                assert remark[0] == "â€œ" and remark[-1] == "â€"
            except AssertionError:
                print(f"ç¬¬{page_i}é¡µç¬¬{element_i+1}è¡Œæ•°æ®å¯èƒ½æœ‰é—®é¢˜")
                remark = "ã€å¯èƒ½å¼‚å¸¸ã€‘" + remark


            list_job_name.append(job_name)
            list_city.append(city)
            list_district.append(district)
            list_time_published.append(time_published)
            list_salary.append(salary)
            list_experience.append(experience)
            list_education.append(education)
            list_company.append(company)
            list_domain.append(domain)
            list_financing.append(financing)
            list_scale.append(scale)
            list_keyword.append(keyword)
            list_remark.append(remark)

            time.sleep(inner_interval)

        print("\n# " + "=" * 50)
        print(f"# ç¬¬ {page_i:>2} é¡µä¿¡æ¯è¯»å–å®Œæ¯•ã€‚")
        print("# " + "=" * 50)

        # ---------------------------------
        # è·³è‡³ä¸‹ä¸€é¡µ
        # ---------------------------------

        # å®šä½â€œä¸‹ä¸€é¡µâ€æŒ‰é’®
        next_page = driver.find_element_by_xpath('//*[@id="s_position_list"]/div[2]/div/span[@action="next"]')

        # ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€æŒ‰é’®
        next_page.click()

        time.sleep(outer_interval)

    # ---------------------------------
    # åˆ›å»º Pandas DataFrame å¯¹è±¡
    # ---------------------------------

    df = pd.DataFrame({
        "èŒä¸š": list_job_name,
        "åŸå¸‚": list_city,
        "åŒºåŸŸ": list_district,
        "å‘å¸ƒæ—¶é—´": list_time_published,
        "æœˆè–ª": list_salary,
        "ç»éªŒéœ€æ±‚": list_experience,
        "å­¦å†éœ€æ±‚": list_education,
        "ä¼ä¸š": list_company,
        "é¢†åŸŸ": list_domain,
        "èèµ„": list_financing,
        "äººå‘˜è§„æ¨¡": list_scale,
        "å…³é”®è¯": list_keyword,
        "å¤‡æ³¨": list_remark,
    })

    return df

# ========================================
# å¯¼å‡ºä¸º Excel æ–‡ä»¶
# ========================================

# df = scrape_and_process()
#
# print("çˆ¬å–ç»“æœï¼š\n", df)
#
# df.to_excel('lagou_data.xlsx', sheet_name=f'{job_key}', index=False)

# ========================================
# ç»“æŸç¨‹åº
# ========================================

# if quit:
#     driver.quit()

# print("\nç¨‹åºè¿è¡Œå®Œæ¯•ã€‚")

"""
ğŸš§å¾…åŠ å…¥åŠŸèƒ½ï¼š

ä½¿ç”¨ While å¾ªç¯ï¼Œç”¨æˆ·çˆ¬å®Œä¸€ä¸ªå…³é”®è¯ä¹‹åä»¥å¦ä¸€ä¸ªå…³é”®è¯ç»§ç»­æ”¶é›†ä¿¡æ¯
ï¼ˆæ¯”å¦‚ï¼Œæœå®Œâ€œPythonâ€ä¹‹ååˆæƒ³æœâ€œäººå·¥æ™ºèƒ½â€äº†ï¼Œåˆæ‡’å¾—å†å¯åŠ¨ä¸€æ¬¡ç¨‹åºï¼‰ï¼Œ
ç»“æœå­˜å…¥åŒä¸€ Excel æ–‡æ¡£çš„æ–°ä¸€å¼  sheet ä¸­ã€‚
"""