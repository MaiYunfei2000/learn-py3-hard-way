##### 6.1 å¤„ç†æ–‡æœ¬æ•°æ®

#### 6.1.2 ä½¿ç”¨è¯åµŒå…¥

## å°†å•è¯ä¸å‘é‡å…³è”è¿˜æœ‰å¦ä¸€ç§å¸¸ç”¨çš„å¼ºå¤§æ–¹æ³•ï¼Œå°±æ˜¯ä½¿ç”¨å¯†é›†çš„è¯å‘é‡ï¼ˆword vectorï¼‰ï¼Œä¹Ÿå«è¯åµŒå…¥ï¼ˆword embeddingï¼‰

"""
è·å–è¯åµŒå…¥æœ‰ä¸¤ç§æ–¹æ³•ï¼š

- åœ¨å®Œæˆä¸»ä»»åŠ¡ï¼ˆæ¯”å¦‚æ–‡æ¡£åˆ†ç±»æˆ–æƒ…æ„Ÿé¢„æµ‹ï¼‰çš„åŒæ—¶å­¦ä¹ è¯åµŒå…¥ã€‚
- åœ¨ä¸åŒäºå¾…è§£å†³é—®é¢˜çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šé¢„è®¡ç®—å¥½è¯åµŒå…¥ï¼Œç„¶åå°†å…¶åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚ï¼ˆé¢„è®­ç»ƒè¯åµŒå…¥(pretrained word embedding)ï¼‰
"""



### 6-5 å°†ä¸€ä¸ªEmbeddingå±‚å®ä¾‹åŒ–

from keras.layers import Embedding

## Embeddingå±‚è‡³å°‘éœ€è¦ä¸¤ä¸ªå‚æ•°ï¼šæ ‡è®°çš„ä¸ªæ•°ï¼ˆè¿™é‡Œæ˜¯1000ï¼Œå³æœ€å¤§å•è¯ç´¢å¼•+1ï¼‰å’ŒåµŒå…¥çš„ç»´åº¦ï¼ˆè¿™é‡Œæ˜¯64ï¼‰
    # æ ‡è®°ä¸ªæ•°ã€æœ€å¤§å•è¯ç´¢å¼•æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ+1æ˜¯ä¸ºä»€ä¹ˆï¼ŸğŸš§
        # å“¦ï¼Œæ ‡è®°åº”è¯¥å°±æ˜¯å®˜æ–¹æ–‡æ¡£ä¸­çš„ index ã€‚
    # åµŒå…¥çš„ç»´åº¦åˆæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿè¿™ä¸ªå› ç´ ä¼šå½±å“ä»€ä¹ˆï¼ŸğŸš§
# åˆ›å»ºä¸€ä¸ªç±»Embeddingçš„å¯¹è±¡
embedding_layer = Embedding(1000, 64)
# ä¸çŸ¥é“æ˜¯åŸä½œè¿˜æ˜¯ç¿»è¯‘çš„é”…ï¼Œè¿˜æ˜¯çœ‹å®˜æ–¹æ–‡æ¡£å¥½æ‡‚â€¦â€¦
    # 1000 å°±æ˜¯åªèƒ½æœ‰ 1000 ç§è¯ï¼Œ64 å°±æ˜¯ Embedding å±‚è¾“å‡ºçš„å¼ é‡çš„ç¬¬2è½´çš„ç»´åº¦æ•°ã€‚

# print(embedding_layer)

# https://keras.io/layers/embeddings/
# keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)
    # Turns positive integers (indexes) into dense vectors of fixed sizeï¼ˆå°†æ­£æ•´æ•°ï¼ˆç´¢å¼•â€”â€”æ¯”å¦‚è¯´å•è¯ç´¢å¼•ï¼ŒMYFæ³¨ï¼‰ä»¬è½¬æ¢ä¸ºç‰¹å®š size çš„å¯†é›†å‘é‡ï¼‰. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]] æ˜¾ç„¶ï¼Œè¿™é‡ŒäºŒç»´å˜æˆä¸‰ç»´äº†ã€‚ï¼ˆä¸ºä»€ä¹ˆåè€…ç›¸è¾ƒå‰è€…æ˜¯ dense çš„ï¼ŸğŸš§ï¼‰
    # This layer can only be used as the first layer in a model.
    
    # input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.
    # output_dim: int >= 0. Dimension of the dense embedding.
    # å…¶å®ƒå‚æ•°çš„è¯´æ˜ç•¥ã€‚

"""å®˜æ–¹çš„ä¾‹å­ï¼š
model = Sequential()
model.add(Embedding(1000, 64, input_length=10))
# the model will take as input an integer matrix of size (batch, input_length). ï¼ˆä¹Ÿå°±æ˜¯ (1000, 64) å’¯ï¼Ÿä¸å¯¹ï¼Œ1000çš„æ„æ€ä¸æ˜¯è¿™ä¸ªï¼Œè§ä¸‹ä¸€è¡Œã€‚ï¼‰
# the largest integer (i.e. word index) in the input should be
# no larger than 999 (vocabulary size). å™¢ï¼ŒåŸæ¥æ˜¯è¿™ä¸ªæ„æ€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™é™åˆ¶äº†è¾“å…¥çš„å¼ é‡ä¸­çš„å…ƒç´ åªèƒ½æ˜¯0~999çš„æ•´æ•°ï¼ˆä¹Ÿå°±æ˜¯è¯´æœ€å¤šæœ‰1000ç§å•è¯ï¼‰ã€‚
# now model.output_shape == (None, 10, 64), where None is the batch dimension.

input_array = np.random.randint(1000, size=(32, 10))
# ç”Ÿæˆå…ƒç´ å€¼ä¸º 1000 ä»¥ä¸‹çš„éšæœºæ•°çŸ©é˜µï¼Œå½¢çŠ¶ä¸º32*10
# https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.random.randint.html

model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
assert output_array.shape == (32, 10, 64)

>>> output_array.shape
(32, 10, 64)

# å‘ç°äº†ä»€ä¹ˆæ²¡æœ‰ï¼Ÿåªè€ƒè™‘å½¢çŠ¶çš„è¯ï¼Œ
# è¿™ä¸ª 3D å¼ é‡å°±æ˜¯ç”± 64 ä¸ª 2D å¼ é‡å æˆçš„ä¸€ä¸ªâ€œé•¿æ–¹ä½“â€ã€‚

"""

### 6-6 åŠ è½½IMDBæ•°æ®ï¼Œå‡†å¤‡ç”¨äºEmbeddingå±‚

from keras.datasets import imdb
from keras import preprocessing
## ä½œä¸ºç‰¹å¾çš„å•è¯ä¸ªæ•°
max_features = 10000
## åœ¨è¿™ä¹ˆå¤šå•è¯åæˆªæ–­æ–‡æœ¬ï¼ˆè¿™äº›å•è¯éƒ½å±äºå‰max_featuresä¸ªæœ€å¸¸è§çš„å•è¯ï¼‰
maxlen = 20

# https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification
(x_train, y_train), (x_test, y_test) = imdb.load_data(
    ## å°†æ•°æ®åŠ è½½ä¸ºæ•´æ•°åˆ—è¡¨
    num_words=max_features)

print("x_train[0]:", x_train[0])
print("y_train:", y_train)
print("x_train.shape, y_train.shape:", x_train.shape, y_train.shape)

## å°†æ•´æ•°åˆ—è¡¨è½¬æ¢æˆå½¢çŠ¶ä¸º(samples, maxlen)çš„äºŒç»´æ•´æ•°å¼ é‡
# https://keras.io/preprocessing/sequence/ ç„¶åæ‰‹åŠ¨æŸ¥æ‰¾ pad_sequences
    # pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)
    # è¿™ä¸ªå‡½æ•°å°†[å…·æœ‰num_samplesä¸ªæ•´æ•°åºåˆ—ï¼ˆåˆ—è¡¨ï¼‰]çš„åˆ—è¡¨ transforms ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º (num_samples, num_timesteps) çš„ 2D NumPy array ã€‚
    # å¦‚æœæä¾›äº† maxlen å‚æ•°çš„å€¼ï¼Œ num_timesteps å€¼å°±æ˜¯ maxlen å€¼ï¼Œå¦åˆ™ï¼Œåˆ™æ˜¯æœ€é•¿çš„é‚£æ¡åºåˆ—çš„é•¿åº¦ã€‚
    # é•¿åº¦å°äº num_timesteps çš„åºåˆ—ä¼šå¡«è¡¥ä¸Š value å€¼ï¼ˆé»˜è®¤ä¸º0ï¼‰ã€‚ä¾‹å¦‚ maxlen=5ï¼Œè€ŒæŸåˆ—è¡¨ä¸­çš„æŸæ¡åºåˆ—æ˜¯ [2, 3, 3]ï¼Œé‚£ä¹ˆè¿™ä¸ªåºåˆ—ä¼šå˜æˆ [2, 3, 3, 0, 0]ï¼ˆä¸ä¸€å®šæ˜¯åœ¨åé¢å¡«è¡¥ï¼Œè§ä¸‹ä¸€è¡Œï¼‰ã€‚
    # é•¿åº¦å¤§äº num_timesteps çš„åºåˆ—åˆ™ä¼šè¢«æˆªæ–­ï¼Œè‡³äºæ€ä¹ˆæˆªæ–­ï¼ˆåœ¨å“ªä¸ªä½ç½®æˆªæ–­ï¼‰åˆ™ä¾æ¬¡å–å†³å‚æ•° truncatingï¼Œè¡¥å…¨ï¼ˆè§ä¸Šä¸€è¡Œï¼‰çš„æ–¹å¼åˆ™å–å†³äº padding å‚æ•°ã€‚è¿™ä¸¤ä¸ªå‚æ•°çš„å€¼é»˜è®¤éƒ½æ˜¯ 'pre' ã€‚
        # padding å‚æ•°ï¼š'pre'åœ¨å‰é¢å¡«è¡¥ï¼›'post'åœ¨åé¢å¡«è¡¥ã€‚
        # truncating å‚æ•°ï¼š'pre'åˆ‡å‰²æ‰å‰é¢çš„ï¼›'post'åˆ‡å‰²æ‰åé¢çš„ã€‚
    """ä½ æ£é¼“æ£é¼“å°±æ˜ç™½äº†ï¼š
    >>> a = [[1,1,1]]
    >>> a_i = preprocessing.sequence.pad_sequences(a, maxlen=5)
    >>> a_i
    array([[0, 0, 1, 1, 1]], dtype=int32)
    >>> a_ii = preprocessing.sequence.pad_sequence(a, maxlen=5, padding='post', value=3.0)
    >>> a_ii = preprocessing.sequence.pad_sequences(a, maxlen=5, padding='post', value=3.0)
    >>> a_ii
    array([[1, 1, 1, 3, 3]], dtype=int32)
    >>> preprocessing.sequence.pad_sequences(a_ii, maxlen=2, truncating='post')
    array([[1, 1]], dtype=int32)
    """
# maxlenå°±æ˜¯å‰é¢è®¾å®šçš„é‚£ä¸ª20ã€‚è¿™é‡ŒæŠŠç¬¬1è½´å½¢çŠ¶å˜ä¸º20ï¼Œä¹Ÿå°±æ˜¯æŒ‡æˆªå–æ–‡æœ¬çš„å‰20ä¸ªå•è¯å—ï¼Ÿè¿˜æ˜¯ä¼šå¹²ä»€ä¹ˆï¼ˆæŸç§å½¢å¼çš„å‹ç¼©ï¼‰ï¼Ÿ
    # æˆªå–æ–‡æœ¬çš„å‰20ä¸ªå•è¯ã€‚
    # ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·å­åšï¼Ÿä¸ºä»€ä¹ˆæ˜¯20è€Œä¸æ˜¯å…¶å®ƒï¼Ÿä¸ºä»€ä¹ˆæ˜¯æˆªæ‰åé¢çš„ï¼Œè€Œä¸æ˜¯æˆªæ‰å‰é¢çš„ï¼ŸğŸš§
# sampleå‘¢ï¼Ÿè¿™é‡Œå°±æ˜¯æŒ‡è®­ç»ƒé›†ï¼ˆä¸‹ä¸€è¡Œï¼‰å’Œæµ‹è¯•é›†ï¼ˆä¸‹ä¸‹è¡Œï¼‰çš„æ ·æœ¬é‡
x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)
# print("x_train:", x_train)
# print(type(x_train))
# print("x_train.shape[0] == y_train.shape[0]:", x_train.shape[0] == y_train.shape[0])

# æ€ä¹ˆå°†åŸå¼ é‡è½¬æ¢ä¸ºç°åœ¨è¿™ä¸ªæ–°çš„æ•´æ•°å¼ é‡å‘¢ï¼Ÿå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿè¿™æ ·çš„ç¼–ç å½¢å¼å¯ä»¥è¡¨ç¤ºå‡ºä»€ä¹ˆ/å…·æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Ÿ
    # å°±æ˜¯ä¸ºäº†å‹ç¼©æ•°æ®çš„â€œç¨ å¯†åº¦â€ï¼Œç»™æœºå™¨å‡è´Ÿã€‚
    # åŸæœ¬çš„ one-hot å½¢å¼çš„ç¼–ç ä¸­ï¼Œä¸€ä¸ªä¸ªå‘é‡çš„é•¿åº¦æœ‰ä¸€ä¸‡ä¸¤ä¸‡ä¹‹å¤š
        # ï¼ˆé•¿åº¦çš„å¤§å°å¦‚ä½•å–å†³äºä½ æƒ³è®©ç¥ç»ç½‘ç»œæœ‰å¤šå¤§çš„è¯æ±‡é‡ï¼‰ï¼Œ
        # ç„¶åæŠŠå…¶ä¸­ä¸€ä¸ª 0 æ ‡è®°ä¸º 1ï¼Œå°±è®©è¿™ä¸ªå‘é‡æŒ‡ä»£ä¸€ä¸ªå•è¯ã€‚
        # ï¼ˆä½ åº”è¯¥è¿˜è®°å¾—ï¼Œ MNIST é‡Œé¢çš„æ•°å­—çš„è¡¨å¾ç”¨çš„ä¹Ÿæ˜¯ one-hot ç¼–ç ã€‚)
        # äºæ˜¯ï¼Œè¿™æ ·å¤šæŠ˜è…¾ç”µè„‘å•Šï¼

# å‰åæ ·æœ¬è½´ï¼ˆç¬¬0è½´ï¼‰æ²¡æœ‰å‘ç”Ÿå˜åŒ–ï¼Œåªæ˜¯ç¬¬1è½´å˜åŒ–äº†
# é‚£ç¬¬1è½´å‘ç”Ÿäº†ä»€ä¹ˆå‘¢ï¼Ÿå“¦ï¼Œè§£å†³äº†ï¼Œçœ‹å‰é¢ pad_sequences() æ³¨é‡Šã€‚

# åŸæ¥å°±æ˜¯è¿™æ ·å˜›ï¼æ‰€ä»¥è¦è®¤çœŸé˜…è¯»å®˜æ–¹æ–‡æ¡£ï¼æ ¹æœ¬å°±ä¸å¤æ‚ä¸ç„ä¹ï¼



### 6-7 åœ¨IMDBæ•°æ®ä¸Šä½¿ç”¨Embeddingå±‚å’Œåˆ†ç±»å™¨

from keras.models import Sequential
from keras.layers import Flatten, Dense, Embedding

model = Sequential()
## åˆ¶å®šEmbeddingå±‚çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼Œä»¥ä¾¿åé¢å°†åµŒå…¥è¾“å…¥å±•å¹³ã€‚Embeddingå±‚æ¿€æ´»çš„å½¢çŠ¶ä¸º(samples, maxlen, 8)
# è¾“å…¥å¼ é‡æœ‰10000çš„è¯æ±‡é‡ï¼Œè¾“å‡ºå¼ é‡çš„ç¬¬2è½´å…·æœ‰8ä¸ªç»´åº¦ï¼Œè¾“å…¥å¼ é‡çš„ç¬¬1ä¸ªè½´æœ‰maxlenåŠ20ä¸ªç»´åº¦
model.add(Embedding(10000, 8, input_length=maxlen))

## å°†ä¸‰ç»´çš„åµŒå…¥å¼ é‡å±•å¹³æˆå½¢çŠ¶ä¸º (samples, maxlen * 8) çš„äºŒç»´å¼ é‡
# è¿™ä¸ª Flatten å±‚ç´§éš Embedding å±‚å…¶åï¼Œ
    # æ¥æ”¶ Embedding å±‚çš„è¾“å…¥ä½œä¸º Flatten å±‚çš„è¾“å‡ºã€‚
model.add(Flatten())
# è¿™ä¸ªå±‚åˆ°åº•æ˜¯å¹²ä»€ä¹ˆçš„å‘¢ï¼Ÿ
# https://keras.io/layers/core/ æœç´¢ Flatten
# keras.layers.Flatten(data_format=None)
    # Flattens the input. Does not affect the batch size.
    # ä¾‹å­ï¼š
    """
    model = Sequential()
    model.add(Conv2D(64, (3, 3),
                     input_shape=(3, 32, 32), padding='same',))
    # now: model.output_shape == (None, 64, 32, 32)

    model.add(Flatten())
    # now: model.output_shape == (None, 65536)
    """
# ä¸ºä»€ä¹ˆè¦å±•å¹³æˆäºŒç»´å¼ é‡å‘¢ï¼Ÿå› ä¸ºç¥ç»ç½‘ç»œåªèƒ½æ¥æ”¶ä¸€ç»´å¼ é‡é¸­ï¼



## åœ¨ä¸Šé¢æ·»åŠ åˆ†ç±»å™¨
model.add(Dense(1, activation='sigmoid'))
# æˆ‘å·²ç»ç–‘æƒ‘å¾ˆä¹…äº†ï¼Œ Dense å±‚æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ
# https://keras.io/layers/core/ ç¬¬ä¸€ä¸ªæ¨¡å—å°±æ˜¯ Dense
# keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
# Just your regular densely-connected NN layer. ï¼ˆåªæ˜¯å¸¸è§„çš„ç´§å¯†è¿æ¥çš„ç¥ç»ç½‘ç»œå±‚ã€‚ï¼‰
    # å™¢ï¼Œæˆ‘æ‡‚äº†ï¼è¿™ç©æ„å°±æ˜¯ä¸€ä¸ªæ™®æ™®é€šé€šçš„å±‚ã€‚æ¯”å¦‚è¯´ MNIST æ•°æ®é›†ç”¨åˆ°ç»å…¸ç¥ç»ç½‘ç»œçš„æœ€åä¸€å±‚å°±æ˜¯é…±ç´«çš„ä¸œè¥¿ï¼å®ƒå°±å•çº¯å†ç”¨ Sigmoid æ¿€æ´»å‡½æ•°å¤„ç†ä¸€æ¬¡ä¿¡æ¯å°±åå‡ºå»äº†ï¼Œä¸ä¼šæœ‰ä»€ä¹ˆèŠ±é‡Œèƒ¡å“¨çš„å¥‡æ€ªä¸œè¥¿ï¼

# ç¼–è¯‘æ¨¡å‹
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
# print a summary representation of model
# https://keras.io/models/about-keras-models/
model.summary()

history = model.fit(x_train, y_train,
                    epochs=10,
                    batch_size=32,
                    validation_split=0.2)
                    
                    

# ä¸€è„¸æ‡µé€¼â€¦â€¦

# ä¸¤ä¸ªæœˆåå†æ¥é‡æ–°ç†äº†ä¸€éï¼Œä¸æ‡µé€¼å•¦ï¼ï¼ï¼ 200412