import time

##### 6.1 Â§ÑÁêÜÊñáÊú¨Êï∞ÊçÆ

#### 6.1.3 Êï¥ÂêàÂú®‰∏ÄËµ∑Ôºö‰ªéÂéüÂßãÊñáÊú¨Âà∞ËØçÂµåÂÖ•

### 1. ‰∏ãËΩΩIMDBÊï∞ÊçÆÁöÑÂéüÂßãÊñáÊú¨

### 6-8 Â§ÑÁêÜIMDBÂéüÂßãÊï∞ÊçÆÁöÑÊ†áÁ≠æ

import os

imdb_dir = 'aclImdb'
# [os.path --- Â∏∏Áî®Ë∑ØÂæÑÊìç‰Ωú ‚Äî Python 3.8.2rc1 ÊñáÊ°£](https://docs.python.org/zh-cn/3/library/os.path.html?highlight=os%20path%20join#os.path.join)
# ÁÑ∂ËÄåÂà∞Â∫ïÊòØ‰ªÄ‰πàÊÑèÊÄù‚Ä¶‚Ä¶Ôºü
train_dir = os.path.join(imdb_dir, 'train')

labels = []
texts = []

# for_loop_indicator = 0 # debug use
for label_type in ['neg', 'pos']:
    dir_name = os.path.join(train_dir, label_type)
    # https://docs.python.org/zh-cn/3/library/os.html?highlight=os%20listdir#os.listdir
    # ËøîÂõû‰∏Ä‰∏™ÂàóË°®ÔºåËØ•ÂàóË°®ÂåÖÂê´‰∫Ü path ‰∏≠ÊâÄÊúâÊñá‰ª∂‰∏éÁõÆÂΩïÁöÑÂêçÁß∞„ÄÇËØ•ÂàóË°®Êåâ‰ªªÊÑèÈ°∫Â∫èÊéíÂàóÔºåÂπ∂‰∏î‰∏çÂåÖÂê´ÁâπÊÆäÊù°ÁõÆ '.' Âíå '..'ÔºåÂç≥‰ΩøÂÆÉ‰ª¨Á°ÆÂÆûÂú®ÁõÆÂΩï‰∏≠Â≠òÂú®
    # ÊØîÂ¶Ç 2644_8.txt ÔºåÊ≠§ÂàóË°®ÈïøÂ∫¶‰∏∫ 25000 ÔºåÂç≥ÂàóË°®Âê´Êúâ 25000 ‰∏™Êñá‰ª∂Âêç
    # print("os.listdir(dir_name)", os.listdir(dir_name))
    # print("os.listdir(dir_name)[0]", os.listdir(dir_name)[0])
    for fname in os.listdir(dir_name):
        
        if fname[-4:] == '.txt':
            f = open(os.path.join(dir_name, fname))
            texts.append(f.read())
            f.close()
            if label_type == 'neg':
                labels.append(0)
            else:
                labels.append(1)
        
    # for_loop_indicator += 1 # debug use
    # print("for_loop_indicator", for_loop_indicator) # debug use
    # print("labels", labels) # debug use
    # print("texts", texts) # debug use
    # time.sleep(0.01)
    # break # debug use

# ‰πüÂèØÂÖàÊää‰∏äËø∞Ê®°ÂùóÂΩìÊàê‰∏Ä‰∏™ÈªëÁÆ±Â≠êÔºåÂè™Áü•ÈÅìÂÆÉÊääÂéüÂßãÊï∞ÊçÆÂ§ÑÁêÜÂ§ÑÁêÜ‰∏Ä‰∏ãÂèòÊàêÊàë‰ª¨ÂêéÈù¢ÂèØ‰ª•Â°ûÁªôÁ•ûÁªèÁΩëÁªúÁî®ÁöÑÊï∞ÊçÆ„ÄÇ
# ËæìÂá∫ÂèòÈáèÁúãÁúãÈïøÂï•Ê†∑Â∞±Ë°å‰∫Ü„ÄÇüöß‰ø°ÊÅØÈáèÂ§™Â§ßÔºåÂú®forÂæ™ÁéØÈáåËØï

# Âì¶Ôºålabel Â∞±ÊòØÂ∫èÂè∑ÂêßÔºü‰∏çÊòØÔºåÊòØÊ≠£Ë¥üÊÉÖÊÑüÊ†áÁ≠æ
# texts ËÆ∞ÂΩïÁöÑÊòØÂéüÊñá

### 2. ÂØπÊï∞ÊçÆËøõË°åÂàÜËØç

### 6-9 ÂØπIMDBÂéüÂßãÊï∞ÊçÆÁöÑÊñáÊú¨ËøõË°åÂàÜËØç

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np

# [Text Preprocessing - Keras Documentation](https://keras.io/preprocessing/text/)
# ‚ÜëÂèØÊü•ÁúãÁ±ª Tokenizer ÂèäÂÖ∂ÊñπÊ≥ïÁöÑ‰ªãÁªç

## Âú® 100 ‰∏™ÂçïËØçÂêéÊà™Êñ≠ËØÑËÆ∫
maxlen = 100
## Âú® 200 ‰∏™Ê†∑Êú¨‰∏äËÆ≠ÁªÉ
training_samples = 200
## Âú® 10000 ‰∏™Ê†∑Êú¨‰∏äÈ™åËØÅ
    # ÔºüÔºüÔºüËøô‰πàÂº∫ÁöÑÈ©¨Ôºü
validation_samples = 10000
## Âè™ËÄÉËôëÊï∞ÊçÆÈõÜ‰∏≠Ââç 10000 ‰∏™ÊúÄÂ∏∏ËßÅÁöÑÂçïËØçÔºàËØ∑Ê≥®ÊÑèÊñ≠Âè•ÔºöÊï∞ÊçÆÈõÜ'‰∏≠'Ââç10000‰∏™Ôºâ
max_words = 10000

# Âª∫Á´ã‰∏Ä‰∏™ÂàÜËØçÂô®ÁöÑÂÆû‰æã
tokenizer = Tokenizer(num_words=max_words)
# ‰ª• texts ÔºàÂç≥Ôºâ ‰∏∫ÂèÇÊï∞Ôºå‰∏¢ÁªôËøô‰∏™ÂàÜËØçÂô®Â§ÑÁêÜ
    # ÂÖ∑‰ΩìÂèëÁîü‰∫Ü‰ªÄ‰πàÔºüÔºüÔºüüöß
    # [tf.keras.preprocessing.text.Tokenizer ¬†|¬† TensorFlow Core v2.1.0](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# sequences = [[...], ..., [..., 4285], [7395, 4322, 6, 3, 1160, 4888, 32, 3374, 3, 988, 4, 8213, 3, 746, 16, 168, 712, 9, 44, 74, 28, 4, 58, 155, 511, 105, 234, 9, 382, 43, 10, 1744, 5, 856, 3031, 326, 15, 1, 19, 5, 213, 43, 8, 285, 2709, 681, 10, 241, 21, 581, 7, 7, 45, 4045, 1167, 23, 2294, 47, 23, 108, 81, 34, 178, 11, 422, 8, 285, 2709, 108, 81, 178, 5, 137, 5, 3161, 16, 11, 174, 2, 11, 226, 108, 81, 178, 5, 398, 3, 8007, 1036, 4, 11, 19, 8, 65, 1, 174, 6, 2092, 1, 625, 2, 455, 9218, 1, 19, 6, 3, 3777, 8969, 108, 25, 458, 1801, 589, 758, 260, 1431, 5, 1, 1027]]
# >>> np.array(sequences).shape
# (25000,)
"""
‰ªéÁªìÊûúÊù•ÁúãÔºåËøô‰∏™ÂàÜËØçÂô®Â∞ÜÊâÄÊúâÊñáÊú¨ÈÉΩÂàÜËØçÂπ∂ËΩ¨Êç¢Êàê‰∫ÜÊï∞Â≠ó„ÄÇ
‰∏çËøáËøô‰∏™Â∫èÂàóÁöÑÂΩ¢Áä∂‰ªçÁÑ∂Ê≤°ÊúâÂèòÔºå‰ªçÁÑ∂Âê´Êúâ 25000 Êù°ÂàóË°®
"""
# ËØ∑ÁúãÂÆòÊñπÊñáÊ°£‰ªãÁªçÔºàÈìæÊé•Âú®È°∂ÈÉ®ÔºâÔºö
# By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized.
# ÁõÆÊµã max_words ËØçÈ¢ë‰ª•Â§ñÁöÑÂçïËØç‰ºöË¢´Ê∂àÈô§ÔºåÂ∞±Ë∑üÊ†áÁÇπÁ¨¶Âè∑ÔºàÂçïËØç‰∏≠ÁöÑÂçïÂºïÂè∑Èô§Â§ñÔºâÂíåÁ©∫Ê†º‰∏ÄÊ†∑

# ‰∏ÄÂè•ËØùÔºåËøô‰∏™ Tokenizer Â∞±Â∏Æ‰Ω†ËΩªÊùæÊêûÂÆö‰∫ÜÊñáÊú¨ÂàÜËØçÂ∑•‰ΩúÂï¶ÔºåÊ†πÊú¨‰∏çÁî®Ëá™Â∑±ÈÄ†ËΩÆÂ≠ê
# ÂΩìÁÑ∂Ôºå‰∏≠ÊñáÁöÑËØùËøòÊòØË¶ÅÂè¨Âî§ jieba ‰∫Ü

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index)) # Found 88582 unique tokens.
# Ëøô‰∏™ word_index ÊòØ‰∏™ÈïøÂ∫¶ 88582 ÁöÑ dict
# ËØçÂÖ∏ÁöÑÊØè‰∏™ÈîÆÈÉΩÊòØ‰∏Ä‰∏™Ë¢´ÂàÜÁ¶ªÂá∫ÁöÑÂçïËØçÔºåÁõ∏Â∫îÁöÑÂÄº‰Ωç‰ªé 1 Âà∞ 88582 ÁöÑ int
#ÔºàÈÇ£‰∏∫‰ªÄ‰πàÊúâ 88582 ‰∏™ÔºüÔºüÔºüüöß‰∏çÊòØËØ¥ËØçÈ¢ë 10000 ÂêóÔºåËøòÊòØËØ¥ËøòÊ≤°‰∏¢Âá∫ÂéªÔºåÂè™ÊòØÂÆåÊàê‰∫ÜÂàÜËØçÂ∑•‰ΩúÔºâ

# ‰øÆÂâ™Ë°®ÂæÅÁùÄÂè•Â≠êÁöÑÂàóË°®ÁöÑÈïøÂ∫¶‰∏∫ 100
# Ê†∑Êú¨ÈáèÊ≤°ÂèòÔºåÊ†∑Êú¨Ë¢´‰øÆÂâ™Êï¥ÈΩê‰∫ÜÔºåÂèØ‰ª•Áõ¥Êé•ËΩ¨Êç¢ÊàêÁü©Èòµ
data = pad_sequences(sequences, maxlen=maxlen) # data.shape ‰∏∫ (25000, 100)

# [numpy.asarray ‚Äî NumPy v1.18 Manual](https://numpy.org/doc/stable/reference/generated/numpy.asarray.html?highlight=asarray#numpy.asarray)
    # Âì¶ÔºåË∑ü np.array(array_like) ÊòØ‰∏ÄÊ†∑ÁöÑ
labels = np.asarray(labels)
print('Shape of data tensor:', data.shape) # Shape of data tensor: (25000, 100)
print('Shape of label tensor:', labels.shape) # Shape of label tensor: (25000,)

## Â∞ÜÊï∞ÊçÆÂàíÂàÜ‰∏∫ËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜÔºå‰ΩÜÈ¶ñÂÖàË¶ÅÊâì‰π±È°∫Â∫èÔºåÂõ†‰∏∫‰∏ÄÂºÄÂßãÊï∞ÊçÆ‰∏≠ÁöÑÊ†∑Êú¨ÊòØÊéíÂ•ΩÂ∫èÁöÑ
# ÔºàÂâç 12500 ‰∏∫Ë¥üÈù¢ËØÑËÆ∫ÔºåÂâ©‰Ωô‰∏ÄÂçä‰∏∫Ê≠£Èù¢ËØÑËÆ∫Ôºâ
# [numpy.arange ‚Äî NumPy v1.18 Manual](https://numpy.org/doc/stable/reference/generated/numpy.arange.html?highlight=arange#numpy.arange)
    # Ë∑üÂÜÖÁΩÆÂáΩÊï∞ range() ‰∏ÄÊ†∑ÔºåÂè™‰∏çËøáËæìÂá∫ÁöÑÂèØËø≠‰ª£Á±ªÂûã‰∏∫‰∏ÄÁª¥ array 
    # ‰πüÂ∞±ÊòØËØ¥Ôºåindeces ‰∏∫ array([0, 1, ... , 24999])
indices = np.arange(data.shape[0])
# [numpy.random.shuffle ‚Äî NumPy v1.18 Manual](https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html?highlight=shuffle#numpy.random.shuffle)
    # ÈöèÊú∫‚ÄúÊ¥óÁâå‚Äù‚Äî‚ÄîÊâì‰π±‰ΩçÁΩÆ
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
# ËøôÊ†∑ÁöÑÁ¥¢ÂºïÊòØ‰ªÄ‰πàÈ¨ºÔºåÂì¶ÊúâÁÇπÂç∞Ë±°ÔºåÈ´òÁ∫ßÂàáÁâáÁé©Ê≥ï
# Â∞±ÊòØ‰ª• indices Áõ∏Â∫î‰ΩçÁΩÆÁöÑÁ¥¢ÂºïÊâÄÁîüÊàêÁöÑÊñ∞È°∫Â∫èËæìÂá∫‰∏Ä‰∏™Âº†Èáè
"""
>>> a = np.array([1, 2, 3])
>>> a
array([1, 2, 3])
>>> indices = [1, 2, 0]
>>> b = a[indices]
>>> b
array([2, 3, 1])
"""

# ÁÑ∂ÂêéÊòØÊó•Â∏∏ÁöÑÂàíÂàÜËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜ
x_train = data[:training_samples] # shape: (200, 100)
y_train = labels[:training_samples] # shape: (200,)
x_val = data[training_samples:training_samples+validation_samples] # shape: (10000, 100)
y_val = labels[training_samples:training_samples+validation_samples] # shape: (10000, 100)
# Âí¶ÔºåÈÇ£Â∞±ÊòØÊ≤°ÊúâÂÖÖÂàÜÂà©Áî®Êï∞ÊçÆÂíØÔºü


### 3. ‰∏ãËΩΩ GloVe ËØçÂµåÂÖ•

# [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)


### 4. ÂØπÂµåÂÖ•ËøõË°åÈ¢ÑÂ§ÑÁêÜ

### 6-10 Ëß£Êûê GloVe ËØçÂµåÂÖ•Êñá‰ª∂

# Âú®ÂêéÁª≠ËøáÁ®ã‰∏≠ÊûÑÂª∫‰∏Ä‰∏™Â∞ÜÂçïËØçÔºàÂ≠óÁ¨¶‰∏≤ÔºâÊò†Â∞Ñ‰∏∫ÂÖ∂ÂêëÈáèË°®Á§∫ÔºàÊï∞ÂÄºÂêëÈáèÔºâÁöÑÁ¥¢Âºï

glove_dir = 'glove.6B'

embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))
for line in f:
    # Â∞ÜËøô‰∏ÄË°åÊñáÊú¨‰ª•Á©∫Ê†º‰∏∫ÂàÜÈöîÁ¨¶ÔºåÂàáÂâ≤ÊñáÊú¨ÁîüÊàê‰∏Ä‰∏™ÂàóË°®
    values = line.split()
    # ÂàóË°®Á¨¨0‰∏™Êï∞ÊçÆÊòØÂçïËØçÊ†áËÆ∞
    word = values[0]
    # Â∞ÜÁ¨¨1‰∏™Êï∞ÊçÆËµ∑ÁöÑÂàóË°®ËΩ¨Êç¢‰∏∫ numpy ÁöÑ 32 ‰ΩçÊµÆÁÇπÊï∞Êï∞ÁªÑ
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))
# Found 400000 word vectors.

# ÂØπ‰∫éÊï∞ÊçÆÈõÜÔºåËá™Â∑±Êù•ÊâìÂç∞ÊâìÂç∞ÁúãÊòØÊÄé‰πàÂõû‰∫ãÔºåÊàñËÄÖÂéªÊñáÊú¨ÊñáÊ°£ÈáåÁúãÁúã
# ‰∏çÔºåËøòÊòØÂà´ÊâìÂºÄÊñáÊú¨ÊñáÊ°£‰∫ÜÔºåÁîµËÑë‰ºöÂç°Ê≠ª‚Ä¶‚Ä¶
# ËøôÊòØÊñáÊ°£ÁöÑÁ¨¨‰∏ÄË°åÔºö
# the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581

# >>> embeddings_index['the']
# array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,
#        -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,
#         0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,
#        -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,
#         0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,
#        -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,
#         0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,
#         0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,
#        -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,
#        -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,
#        -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,
#        -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,
#        -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,
#        -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,
#        -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,
#         0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,
#        -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)

### 6-11 ÂáÜÂ§á GloVe ËØçÂµåÂÖ•Áü©Èòµ

embedding_dim = 100

# 10000 * 100 ÁöÑÂÖ®Èõ∂Áü©Èòµ
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    # ÂØπ‰∫éÂâç 9999 ‰∏™ËØçÔºö
    if i < max_words:
        # ÁîüÊàêÂµåÂÖ•ËØçÂêëÈáè
        # Á¥¢ÂºïÂÖ≥Á≥ªÔºö word_index ÁöÑÈîÆ => embeddings_index ‰∏≠Áõ∏ÂêåÁöÑÈîÆ => embeddings_index ‰∏≠Áõ∏Â∫îÁöÑÂÄºÔºåÂç≥ÂµåÂÖ•ËØçÂêëÈáè
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            ## ÂµåÂÖ•Á¥¢Âºï (embeddings_index) ‰∏≠Êâæ‰∏çÂà∞ÁöÑËØçÔºåÂÖ∂ÂµåÂÖ•ÂêëÈáèÂÖ®‰∏∫ 0
            embedding_matrix[i] = embedding_vector
# Áî±Ê≠§ÔºåÂ∞±ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂèØ‰ª•Âä†ËΩΩÂà∞ Embedding Â±Ç‰∏≠ÁöÑÂµåÂÖ•Áü©Èòµ

### 5. ÂÆö‰πâÊ®°Âûã

### 6-12 Ê®°ÂûãÂÆö‰πâ

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense

# Êñ∞Âª∫‰∏Ä‰∏™ Sequential() Á±ªÁöÑÂØπË±°/ÂÆû‰æãÔºåÂç≥Êñ∞Âª∫‰∏Ä‰∏™Á∫øÊÄßÂ†ÜÂè†‰∏ÄÂ±ÇÂ±ÇÁ•ûÁªèÁΩëÁªúÁöÑÊ®°Âûã
# [Guide to the Sequential model - Keras Documentation](https://keras.io/getting-started/sequential-model-guide/)
model = Sequential()
# Ëøô‰∏™ Embedding Â±ÇËæìÂá∫ÁöÑÁü©ÈòµÂê´Êúâ 10000 ËØçÈ¢ëÔºåËæìÂá∫Êï∞ÊçÆÁöÑÂµåÂÖ•Áª¥Â∫¶‰∏∫ 100 Ôºå 
# ËæìÂÖ•ÂêëÈáèÁöÑÁª¥Â∫¶‰∏∫ 100
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()


### 6. Âú®Ê®°Âûã‰∏≠Âä†ËΩΩ GloVe ÂµåÂÖ•

### 6-13 Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑËØçÂµåÂÖ•Âä†ËΩΩÂà∞ Embedding Â±Ç‰∏≠

model.layers[0].set_weights([embedding_matrix])
## ÂÜªÁªì Embedding Â±ÇÔºö‰∏çÂ∫îÂú®ËÆ≠ÁªÉÊó∂Êõ¥Êñ∞È¢ÑËÆ≠ÁªÉÁöÑÈÉ®ÂàÜÔºå‰ª•ÂÖç‰∏¢Â§±ÂÆÉ‰ª¨ÊâÄ‰øùÂ≠òÁöÑ‰ø°ÊÅØ
## ÈöèÊú∫ÂàùÂßãÂåñÁöÑÂ±Ç‰ºöÂºïËµ∑ËæÉÂ§ßÁöÑÊ¢ØÂ∫¶Êõ¥Êñ∞Ôºå‰ºöÁ†¥ÂùèÂ∑≤ÁªèÂ≠¶Âà∞ÁöÑÁâπÂæÅ
model.layers[0].trainable = False


### 7. ËÆ≠ÁªÉÊ®°Âûã‰∏éËØÑ‰º∞Ê®°Âûã

### 6-14 ËÆ≠ÁªÉ‰∏éËØÑ‰º∞

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])
history = model.fit(x_train, y_train,
                    epochs=10,
                    batch_size=32,
                    validation_data=(x_val, y_val))
model.save_weights('pre_trained_glove_model.h5')


### 6-15 ÁªòÂà∂ÁªìÊûú

import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

### 6-16 Ëá≥ 6-18 Áï•