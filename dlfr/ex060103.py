import time

##### 6.1 å¤„ç†æ–‡æœ¬æ•°æ®

#### 6.1.3 æ•´åˆåœ¨ä¸€èµ·ï¼šä»åŸå§‹æ–‡æœ¬åˆ°è¯åµŒå…¥

### 1. ä¸‹è½½IMDBæ•°æ®çš„åŸå§‹æ–‡æœ¬

### 6-8 å¤„ç†IMDBåŸå§‹æ•°æ®çš„æ ‡ç­¾

import os

imdb_dir = 'aclImdb'
# [os.path --- å¸¸ç”¨è·¯å¾„æ“ä½œ â€” Python 3.8.2rc1 æ–‡æ¡£](https://docs.python.org/zh-cn/3/library/os.path.html?highlight=os%20path%20join#os.path.join)
# ç„¶è€Œåˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€â€¦â€¦ï¼Ÿ
train_dir = os.path.join(imdb_dir, 'train')

labels = []
texts = []

# for_loop_indicator = 0 # debug use
for label_type in ['neg', 'pos']:
    dir_name = os.path.join(train_dir, label_type)
    # https://docs.python.org/zh-cn/3/library/os.html?highlight=os%20listdir#os.listdir
    # è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œè¯¥åˆ—è¡¨åŒ…å«äº† path ä¸­æ‰€æœ‰æ–‡ä»¶ä¸ç›®å½•çš„åç§°ã€‚è¯¥åˆ—è¡¨æŒ‰ä»»æ„é¡ºåºæ’åˆ—ï¼Œå¹¶ä¸”ä¸åŒ…å«ç‰¹æ®Šæ¡ç›® '.' å’Œ '..'ï¼Œå³ä½¿å®ƒä»¬ç¡®å®åœ¨ç›®å½•ä¸­å­˜åœ¨
    # æ¯”å¦‚ 2644_8.txt ï¼Œæ­¤åˆ—è¡¨é•¿åº¦ä¸º 25000 ï¼Œå³åˆ—è¡¨å«æœ‰ 25000 ä¸ªæ–‡ä»¶å
    # print("os.listdir(dir_name)", os.listdir(dir_name))
    # print("os.listdir(dir_name)[0]", os.listdir(dir_name)[0])
    for fname in os.listdir(dir_name):
        
        if fname[-4:] == '.txt':
            f = open(os.path.join(dir_name, fname))
            texts.append(f.read())
            f.close()
            if label_type == 'neg':
                labels.append(0)
            else:
                labels.append(1)
        
    # for_loop_indicator += 1 # debug use
    # print("for_loop_indicator", for_loop_indicator) # debug use
    # print("labels", labels) # debug use
    # print("texts", texts) # debug use
    # time.sleep(0.01)
    # break # debug use

# ä¹Ÿå¯å…ˆæŠŠä¸Šè¿°æ¨¡å—å½“æˆä¸€ä¸ªé»‘ç®±å­ï¼ŒåªçŸ¥é“å®ƒæŠŠåŸå§‹æ•°æ®å¤„ç†å¤„ç†ä¸€ä¸‹å˜æˆæˆ‘ä»¬åé¢å¯ä»¥å¡ç»™ç¥ç»ç½‘ç»œç”¨çš„æ•°æ®ã€‚
# è¾“å‡ºå˜é‡çœ‹çœ‹é•¿å•¥æ ·å°±è¡Œäº†ã€‚ğŸš§ä¿¡æ¯é‡å¤ªå¤§ï¼Œåœ¨forå¾ªç¯é‡Œè¯•

# å“¦ï¼Œlabel å°±æ˜¯åºå·å§ï¼Ÿä¸æ˜¯ï¼Œæ˜¯æ­£è´Ÿæƒ…æ„Ÿæ ‡ç­¾
# texts è®°å½•çš„æ˜¯åŸæ–‡

### 2. å¯¹æ•°æ®è¿›è¡Œåˆ†è¯

### 6-9 å¯¹IMDBåŸå§‹æ•°æ®çš„æ–‡æœ¬è¿›è¡Œåˆ†è¯

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np

# [Text Preprocessing - Keras Documentation](https://keras.io/preprocessing/text/)
# â†‘å¯æŸ¥çœ‹ç±» Tokenizer åŠå…¶æ–¹æ³•çš„ä»‹ç»

## åœ¨ 100 ä¸ªå•è¯åæˆªæ–­è¯„è®º
maxlen = 100
## åœ¨ 200 ä¸ªæ ·æœ¬ä¸Šè®­ç»ƒ
training_samples = 200
## åœ¨ 10000 ä¸ªæ ·æœ¬ä¸ŠéªŒè¯
    # ï¼Ÿï¼Ÿï¼Ÿè¿™ä¹ˆå¼ºçš„é©¬ï¼Ÿ
validation_samples = 10000
## åªè€ƒè™‘æ•°æ®é›†ä¸­å‰ 10000 ä¸ªæœ€å¸¸è§çš„å•è¯ï¼ˆè¯·æ³¨æ„æ–­å¥ï¼šæ•°æ®é›†'ä¸­'å‰10000ä¸ªï¼‰
max_words = 10000

# å»ºç«‹ä¸€ä¸ªåˆ†è¯å™¨çš„å®ä¾‹
tokenizer = Tokenizer(num_words=max_words)
# ä»¥ texts ï¼ˆå³ï¼‰ ä¸ºå‚æ•°ï¼Œä¸¢ç»™è¿™ä¸ªåˆ†è¯å™¨å¤„ç†
    # å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿï¼Ÿï¼ŸğŸš§
    # [tf.keras.preprocessing.text.Tokenizer Â |Â  TensorFlow Core v2.1.0](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# sequences = [[...], ..., [..., 4285], [7395, 4322, 6, 3, 1160, 4888, 32, 3374, 3, 988, 4, 8213, 3, 746, 16, 168, 712, 9, 44, 74, 28, 4, 58, 155, 511, 105, 234, 9, 382, 43, 10, 1744, 5, 856, 3031, 326, 15, 1, 19, 5, 213, 43, 8, 285, 2709, 681, 10, 241, 21, 581, 7, 7, 45, 4045, 1167, 23, 2294, 47, 23, 108, 81, 34, 178, 11, 422, 8, 285, 2709, 108, 81, 178, 5, 137, 5, 3161, 16, 11, 174, 2, 11, 226, 108, 81, 178, 5, 398, 3, 8007, 1036, 4, 11, 19, 8, 65, 1, 174, 6, 2092, 1, 625, 2, 455, 9218, 1, 19, 6, 3, 3777, 8969, 108, 25, 458, 1801, 589, 758, 260, 1431, 5, 1, 1027]]
# >>> np.array(sequences).shape
# (25000,)
"""
ä»ç»“æœæ¥çœ‹ï¼Œè¿™ä¸ªåˆ†è¯å™¨å°†æ‰€æœ‰æ–‡æœ¬éƒ½åˆ†è¯å¹¶è½¬æ¢æˆäº†æ•°å­—ã€‚
ä¸è¿‡è¿™ä¸ªåºåˆ—çš„å½¢çŠ¶ä»ç„¶æ²¡æœ‰å˜ï¼Œä»ç„¶å«æœ‰ 25000 æ¡åˆ—è¡¨
"""
# è¯·çœ‹å®˜æ–¹æ–‡æ¡£ä»‹ç»ï¼ˆé“¾æ¥åœ¨é¡¶éƒ¨ï¼‰ï¼š
# By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized.
# ç›®æµ‹ max_words è¯é¢‘ä»¥å¤–çš„å•è¯ä¼šè¢«æ¶ˆé™¤ï¼Œå°±è·Ÿæ ‡ç‚¹ç¬¦å·ï¼ˆå•è¯ä¸­çš„å•å¼•å·é™¤å¤–ï¼‰å’Œç©ºæ ¼ä¸€æ ·

# ä¸€å¥è¯ï¼Œè¿™ä¸ª Tokenizer å°±å¸®ä½ è½»æ¾æå®šäº†æ–‡æœ¬åˆ†è¯å·¥ä½œå•¦ï¼Œæ ¹æœ¬ä¸ç”¨è‡ªå·±é€ è½®å­
# å½“ç„¶ï¼Œä¸­æ–‡çš„è¯è¿˜æ˜¯è¦å¬å”¤ jieba äº†

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index)) # Found 88582 unique tokens.
# è¿™ä¸ª word_index æ˜¯ä¸ªé•¿åº¦ 88582 çš„ dict
# è¯å…¸çš„æ¯ä¸ªé”®éƒ½æ˜¯ä¸€ä¸ªè¢«åˆ†ç¦»å‡ºçš„å•è¯ï¼Œç›¸åº”çš„å€¼ä½ä» 1 åˆ° 88582 çš„ int
#ï¼ˆé‚£ä¸ºä»€ä¹ˆæœ‰ 88582 ä¸ªï¼Ÿï¼Ÿï¼ŸğŸš§ä¸æ˜¯è¯´è¯é¢‘ 10000 å—ï¼Œè¿˜æ˜¯è¯´è¿˜æ²¡ä¸¢å‡ºå»ï¼Œåªæ˜¯å®Œæˆäº†åˆ†è¯å·¥ä½œï¼‰

# ä¿®å‰ªè¡¨å¾ç€å¥å­çš„åˆ—è¡¨çš„é•¿åº¦ä¸º 100
# æ ·æœ¬é‡æ²¡å˜ï¼Œæ ·æœ¬è¢«ä¿®å‰ªæ•´é½äº†ï¼Œå¯ä»¥ç›´æ¥è½¬æ¢æˆçŸ©é˜µ
data = pad_sequences(sequences, maxlen=maxlen) # data.shape ä¸º (25000, 100)

# [numpy.asarray â€” NumPy v1.18 Manual](https://numpy.org/doc/stable/reference/generated/numpy.asarray.html?highlight=asarray#numpy.asarray)
    # å“¦ï¼Œè·Ÿ np.array(array_like) æ˜¯ä¸€æ ·çš„
labels = np.asarray(labels)
print('Shape of data tensor:', data.shape) # Shape of data tensor: (25000, 100)
print('Shape of label tensor:', labels.shape) # Shape of label tensor: (25000,)

## å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œä½†é¦–å…ˆè¦æ‰“ä¹±é¡ºåºï¼Œå› ä¸ºä¸€å¼€å§‹æ•°æ®ä¸­çš„æ ·æœ¬æ˜¯æ’å¥½åºçš„
# ï¼ˆå‰ 12500 ä¸ºè´Ÿé¢è¯„è®ºï¼Œå‰©ä½™ä¸€åŠä¸ºæ­£é¢è¯„è®ºï¼‰
# [numpy.arange â€” NumPy v1.18 Manual](https://numpy.org/doc/stable/reference/generated/numpy.arange.html?highlight=arange#numpy.arange)
    # è·Ÿå†…ç½®å‡½æ•° range() ä¸€æ ·ï¼Œåªä¸è¿‡è¾“å‡ºçš„å¯è¿­ä»£ç±»å‹ä¸ºä¸€ç»´ array 
    # ä¹Ÿå°±æ˜¯è¯´ï¼Œindeces ä¸º array([0, 1, ... , 24999])
indices = np.arange(data.shape[0])
# [numpy.random.shuffle â€” NumPy v1.18 Manual](https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html?highlight=shuffle#numpy.random.shuffle)
    # éšæœºâ€œæ´—ç‰Œâ€â€”â€”æ‰“ä¹±ä½ç½®
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
# è¿™æ ·çš„ç´¢å¼•æ˜¯ä»€ä¹ˆé¬¼ï¼Œå“¦æœ‰ç‚¹å°è±¡ï¼Œé«˜çº§åˆ‡ç‰‡ç©æ³•
# å°±æ˜¯ä»¥ indices ç›¸åº”ä½ç½®çš„ç´¢å¼•æ‰€ç”Ÿæˆçš„æ–°é¡ºåºè¾“å‡ºä¸€ä¸ªå¼ é‡
"""
>>> a = np.array([1, 2, 3])
>>> a
array([1, 2, 3])
>>> indices = [1, 2, 0]
>>> b = a[indices]
>>> b
array([2, 3, 1])
"""

# ç„¶åæ˜¯æ—¥å¸¸çš„åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
x_train = data[:training_samples] # shape: (200, 100)
y_train = labels[:training_samples] # shape: (200,)
x_val = data[training_samples:training_samples+validation_samples] # shape: (10000, 100)
y_val = labels[training_samples:training_samples+validation_samples] # shape: (10000, 100)
# å’¦ï¼Œé‚£å°±æ˜¯æ²¡æœ‰å……åˆ†åˆ©ç”¨æ•°æ®å’¯ï¼Ÿ


### 3. ä¸‹è½½ GloVe è¯åµŒå…¥

# [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)


### 4. å¯¹åµŒå…¥è¿›è¡Œé¢„å¤„ç†

### 6-10 è§£æ GloVe è¯åµŒå…¥æ–‡ä»¶

# åœ¨åç»­è¿‡ç¨‹ä¸­æ„å»ºä¸€ä¸ªå°†å•è¯ï¼ˆå­—ç¬¦ä¸²ï¼‰æ˜ å°„ä¸ºå…¶å‘é‡è¡¨ç¤ºï¼ˆæ•°å€¼å‘é‡ï¼‰çš„ç´¢å¼•

glove_dir = 'glove.6B'

embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))
for line in f:
    # å°†è¿™ä¸€è¡Œæ–‡æœ¬ä»¥ç©ºæ ¼ä¸ºåˆ†éš”ç¬¦ï¼Œåˆ‡å‰²æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªåˆ—è¡¨
    values = line.split()
    # åˆ—è¡¨ç¬¬0ä¸ªæ•°æ®æ˜¯å•è¯æ ‡è®°
    word = values[0]
    # å°†ç¬¬1ä¸ªæ•°æ®èµ·çš„åˆ—è¡¨è½¬æ¢ä¸º numpy çš„ 32 ä½æµ®ç‚¹æ•°æ•°ç»„
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))
# Found 400000 word vectors.

# å¯¹äºæ•°æ®é›†ï¼Œè‡ªå·±æ¥æ‰“å°æ‰“å°çœ‹æ˜¯æ€ä¹ˆå›äº‹ï¼Œæˆ–è€…å»æ–‡æœ¬æ–‡æ¡£é‡Œçœ‹çœ‹
# ä¸ï¼Œè¿˜æ˜¯åˆ«æ‰“å¼€æ–‡æœ¬æ–‡æ¡£äº†ï¼Œç”µè„‘ä¼šå¡æ­»â€¦â€¦
# è¿™æ˜¯æ–‡æ¡£çš„ç¬¬ä¸€è¡Œï¼š
# the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581

# >>> embeddings_index['the']
# array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,
#        -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,
#         0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,
#        -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,
#         0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,
#        -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,
#         0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,
#         0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,
#        -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,
#        -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,
#        -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,
#        -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,
#        -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,
#        -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,
#        -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,
#         0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,
#        -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)

### 6-11 å‡†å¤‡ GloVe è¯åµŒå…¥çŸ©é˜µ

embedding_dim = 100

# 10000 * 100 çš„å…¨é›¶çŸ©é˜µ
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    # å¯¹äºå‰ 9999 ä¸ªè¯ï¼š
    if i < max_words:
        # ç”ŸæˆåµŒå…¥è¯å‘é‡
        # ç´¢å¼•å…³ç³»ï¼š word_index çš„é”® => embeddings_index ä¸­ç›¸åŒçš„é”® => embeddings_index ä¸­ç›¸åº”çš„å€¼ï¼Œå³åµŒå…¥è¯å‘é‡
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            ## åµŒå…¥ç´¢å¼• (embeddings_index) ä¸­æ‰¾ä¸åˆ°çš„è¯ï¼Œå…¶åµŒå…¥å‘é‡å…¨ä¸º 0
            embedding_matrix[i] = embedding_vector
# ç”±æ­¤ï¼Œå°±æ„å»ºäº†ä¸€ä¸ªå¯ä»¥åŠ è½½åˆ° Embedding å±‚ä¸­çš„åµŒå…¥çŸ©é˜µ

### 5. å®šä¹‰æ¨¡å‹

### 6-12 æ¨¡å‹å®šä¹‰

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense

# æ–°å»ºä¸€ä¸ª Sequential() ç±»çš„å¯¹è±¡/å®ä¾‹ï¼Œå³æ–°å»ºä¸€ä¸ªçº¿æ€§å †å ä¸€å±‚å±‚ç¥ç»ç½‘ç»œçš„æ¨¡å‹
# [Guide to the Sequential model - Keras Documentation](https://keras.io/getting-started/sequential-model-guide/)
model = Sequential()
# è¿™ä¸ª Embedding å±‚è¾“å‡ºçš„çŸ©é˜µå«æœ‰ 10000 è¯é¢‘ï¼Œè¾“å‡ºæ•°æ®çš„åµŒå…¥ç»´åº¦ä¸º 100 ï¼Œ 
# è¾“å…¥å‘é‡çš„ç»´åº¦ä¸º 100
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()


### 6. åœ¨æ¨¡å‹ä¸­åŠ è½½ GloVe åµŒå…¥

### 6-13 å°†é¢„è®­ç»ƒçš„è¯åµŒå…¥åŠ è½½åˆ° Embedding å±‚ä¸­

model.layers[0].set_weights([embedding_matrix])
## å†»ç»“ Embedding å±‚ï¼šä¸åº”åœ¨è®­ç»ƒæ—¶æ›´æ–°é¢„è®­ç»ƒçš„éƒ¨åˆ†ï¼Œä»¥å…ä¸¢å¤±å®ƒä»¬æ‰€ä¿å­˜çš„ä¿¡æ¯
## éšæœºåˆå§‹åŒ–çš„å±‚ä¼šå¼•èµ·è¾ƒå¤§çš„æ¢¯åº¦æ›´æ–°ï¼Œä¼šç ´åå·²ç»å­¦åˆ°çš„ç‰¹å¾
model.layers[0].trainable = False


### 7. è®­ç»ƒæ¨¡å‹ä¸è¯„ä¼°æ¨¡å‹

### 6-14 è®­ç»ƒä¸è¯„ä¼°

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])
history = model.fit(x_train, y_train,
                    epochs=10,
                    batch_size=32,
                    validation_data=(x_val, y_val))
model.save_weights('pre_trained_glove_model.h5')


### 6-15 ç»˜åˆ¶ç»“æœ

import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

### 6-16 è‡³ 6-18 ç•¥