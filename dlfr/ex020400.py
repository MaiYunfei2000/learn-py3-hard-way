##### 2.4 神经网络的“引擎”：基于梯度的优化

# ⚠️这里没有可运行的代码。

#### 2.4.2 张量运算的导数：梯度
#### 2.4.3 随机梯度下降
#### 2.4.4 链式求导：反向传播算法

"""
- 梯度：多元函数的导数。其中，多元函数是以张量为自变量的函数。
"""
# 原来就是这样。。。函数与导数的推广。。。

# 而「损失」与「误差」同义（见Tariq的《Python神经网络编程》）
loss_value = f(W)

"""
神经网络学习的目的就是找到一个使损失最小的权重值。这就转化成了微积分问题。

求最值？那么需要确定函数的单调性。于是需要求导函数，判断导函数的大小（在某区间上
大于/小于/等于0）。也需要找到导函数的零点。更多内容略。

以上是数学的范围（解析法）。神经网络稍微大一点，函数表达式就会变得极其复杂而
难以通过直接计算得出，因此需要转化为统计问题，并交给计算机暴力解决。

到了工程上该怎么实现呢？不断给W加一个个△W（也就是「梯度」，函数在某个点上的曲率（一个张量））。
怎样定△Wi呢？通过f'(Wi)的符号和大小。比如「曲率」（“函数导数/斜率”的推广）为正，
我就让deltaW为负。比如曲率（|f'(Wi)|）越大，我就让|deltaWi|越小。即「方向」和
「步长」/「学习率」。

这是一种暴力算法。

这从标量与导数推广到张量与梯度的范围，成了所谓的「梯度下降」。“山谷”就是函数的极值。
多元函数通常有多个极值点，也就是多个“山谷”。随机起点的话，用上述暴力算法随机地运动，
难免会掉落至不是最低的山谷里，也就是找到一个非最值的极值。因此就要涉及到更多问题。
（不是此次的话题）
"""

"""
- 随机梯度下降 (stochastic gradient descent)
    - 小批量SGD：每次迭代时只抽取一个样本及相应目标
    - 批量SGD：另一个极端，每次迭代使用所有数据
    - 真SGD：非小批量的SGD
"""

"""
SGD还有多种变体，如带「动量」的SGD、Adagrad、RMSProp等。

- 动量：来自物理学的隐喻。如果一个小球冲向谷底有足够大的动量，球就会冲出这个山谷到别的山谷。
否则可能停留在不是最低的山谷里。这也就引入了“加速度”的概念。给实践的指导是，更新参数不仅要
考虑当前梯度值，还要考虑之前的更新情况。
"""

"""
- 反向传播/反式微分

因此需要支持「符号微分」框架如TensorFlow来实现神经网络。
"""