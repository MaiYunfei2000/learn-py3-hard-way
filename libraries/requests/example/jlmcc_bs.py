# æœ¬æ¡ˆä¾‹çˆ¬å–çš„é¡µé¢ï¼šè©¹å§†æ–¯Â·éº¦å…‹è±å…°çš„è‘—ä½œç›®å½•
# (https://stanford.edu/~jlmcc/papers/)

import requests
import numpy as np
from bs4 import BeautifulSoup

########################################
#            LOAD THE HTML
########################################

# request the page
response = requests.get('https://stanford.edu/~jlmcc/papers')
# see if success
if response:
    print('\nSuccessfully requested!\n')
else:
    print('\nAn error occurred.\n')

# get the resonse content, obviously it's a HTML document
content = response.text
# print('='*50, content, '='*50)
# print('\nâ†‘â†‘â†‘ WEB CONTENT â†‘â†‘â†‘\n')
# print('type:', type(content), '\n')

########################################
#       EXTRACT THE INFORMATION
########################################

# parse the content to HTML text
soup = BeautifulSoup(content, features="lxml")
# see how does it look like
# print(etree.tostring(html))

# read the all sibling-<p> nodes after the <em> node  (except the last <p>)
"""
using the tricky copying tool of Chrome, we know we want nodes
from '/html/body/p[3]' to '/html/body/p[251]' and exclude [223]
"""
# è‡³å°‘åœ¨è¿™æ¬¡å®ä¾‹ä¸­ï¼Œbs çš„é²æ£’æ€§æ¯” lxml è¦å¥½ï¼Œæ²¡æœ‰åæ‰å€’æ•°å‡ åä¸ª p èŠ‚ç‚¹

def read_text():
    all_text = []
    ls_all_p = list(soup.find_all('p'))
    for i in range(len(ls_all_p)):
        text = ''.join([i for i in ls_all_p[i].stripped_strings])
        all_text.append(text)
    return all_text
# è¯¡å¼‚å¾—ä¸€æ‰¹ï¼Œç”¨'//body/p'åªè¯»åˆ°é•¿ä¸º221çš„åˆ—è¡¨

all_text = read_text()
# print(all_text)

########################################
#        PROCESS THE INFORMATION
########################################

# read the first part (text of <p>, authors, year and title)
def process_part1_text(string):
    """
    input  : (lxml.etree._Element.text) outer <p> node, containing
             information of the authors, year and title
    output : (dict) processed certain information
    """
    # print("string:", repr(string))
    # æŠŠå¥‡æ€ªçš„è·³è¡Œå’Œç©ºæ ¼ç»™å¤„ç†æ‰
    author_year_title = ''.join(string.split('\r\n ')).strip()
    # å†æ¥ä¸€æ¬¡
    author_year_title = ' '.join(author_year_title.split('\r\n')).strip()
    # print("[2]author_year_title:", author_year_title)
    # åˆ†å‰²ä½œè€…ã€å¹´ä»£å’Œæ–‡ç« æ ‡é¢˜
        # åªåœ¨é¦–ä¸ªç¬¦åˆæ¡ä»¶åœ°æ–¹åˆ†å‰²ä¸€æ¬¡ï¼Œé¿å…å‡ºç°æ–‡ç« æ ‡é¢˜ä¹Ÿæœ‰ç›¸åŒå­—ç¬¦çš„æƒ…å½¢
    author_year_title = author_year_title.split(' (', maxsplit=1)
    # æŠŠä½œè€…å•ç‹¬æ‹å‡ºæ¥
    author = author_year_title[0]
    # print("[3]author_year_title:", author_year_title)
    if author_year_title == ['']:
        return None
    # æŠŠå¹´ä»½+æ ‡é¢˜æ‹å‡ºæ¥
    year_title = author_year_title[1]
    # print("[4]year_title:", year_title)
    # ç»§ç»­å¤„ç†å¹´ä»½+æ ‡é¢˜ï¼Œç”¨').'å°†å¹´ä»½å’Œæ ‡é¢˜åˆ†å‰²
        # åªåœ¨é¦–ä¸ªç¬¦åˆæ¡ä»¶åœ°æ–¹åˆ†å‰²ä¸€æ¬¡ï¼Œé¿å…å‡ºç°æ–‡ç« æ ‡é¢˜ä¹Ÿæœ‰ç›¸åŒå­—ç¬¦çš„æƒ…å½¢
    year_title = year_title.split('). ', maxsplit=1)
    if len(year_title) == 1:
        year_title = year_title[0]
    # print("[4.5]year_title:", year_title)
    try:
        year_title = year_title.split(').', maxsplit=1)
    except AttributeError:
        pass
    # æŠŠå¹´ä»½æ‹å‡ºæ¥
    year = year_title[0]
    # æŠŠæ ‡é¢˜æ‹å‡ºæ¥
    # print("[5]year_title", year_title)
    title = year_title[1].strip('.')
    # æŠŠä¸‰ä¸ªä¿¡æ¯åˆå¹¶åˆ°å­—å…¸é‡Œ
    inf_dict = {'author': author,
                 'year': year,
                 'title': title}
    return inf_dict

# write existing information into a list
all_paper_inf = []
for i in range(len(all_text)):
    # print("="*50 + '\n' + str(i) + '\n' + "="*50)
    try:
        all_paper_inf.append(process_part1_text(all_text[i].text))
    except:
        all_paper_inf.append(None)

# print(all_paper_inf)

test_output = open('test.csv', 'w+')
test_output.write('author,year,title\n')

# ğŸš§
for i in range(len(all_paper_inf)):
    try:
        test_output.write(','.join([all_paper_inf[i]['author'],
                                    all_paper_inf[i]['year'],
                                    all_paper_inf[i]['title'] + '\n']))
    except TypeError:
        pass

# waiï¼Œè¿™ä¹ˆå¤šé€—å·çš„æ–‡æœ¬ç”¨ä»€ä¹ˆ csv ï¼Œè¿˜æ˜¯ä¹–ä¹–å­¦ Pandas ç„¶åè¾“å‡º Excel å§ï¼

# read the second part


# ç»“è®ºï¼Œåˆ«å†æ£é¼“ä¸‹å»äº†ï¼Œä½ åªçŸ¥é“ä½ éœ€è¦å­¦è‡ªç„¶è¯­è¨€å¤„ç†_(:Ğ·ã€âˆ )_å¦åˆ™è¿™ç§äº‹æƒ…çœŸçš„æ²¡æ•ˆç‡